#!/usr/bin/env python3
"""
çµ‚æ¿æª¢æ¸¬æ•¸æ“šæº–å‚™è…³æœ¬
Endplate Detection Data Preparation Script

è™•ç†æ–°çš„JSONæ¨™è¨»æ ¼å¼ï¼Œæº–å‚™çµ‚æ¿æª¢æ¸¬è¨“ç·´æ•¸æ“š
"""

import os
import json
import numpy as np
from pathlib import Path
import shutil
from tqdm import tqdm
import argparse

class EndplateDataPreparer:
    """çµ‚æ¿æª¢æ¸¬æ•¸æ“šæº–å‚™å™¨"""
    
    def __init__(self, input_dir, output_dir):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„çµæ§‹
        (self.output_dir / 'images').mkdir(exist_ok=True)
        (self.output_dir / 'annotations').mkdir(exist_ok=True)
        (self.output_dir / 'visualizations').mkdir(exist_ok=True)
    
    def collect_annotations(self):
        """æ”¶é›†æ‰€æœ‰æ¨™è¨»æª”æ¡ˆ"""
        print("ğŸ” æ”¶é›†æ¨™è¨»æª”æ¡ˆ...")
        
        json_files = list(self.input_dir.glob('**/*.json'))
        annotations = []
        
        for json_file in tqdm(json_files, desc="è™•ç†JSONæª”æ¡ˆ"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # é©—è­‰å¿…è¦æ¬„ä½
                if self.validate_annotation(data):
                    annotations.append({
                        'file': json_file,
                        'data': data
                    })
                else:
                    print(f"âš ï¸ è·³éç„¡æ•ˆæª”æ¡ˆ: {json_file}")
            
            except Exception as e:
                print(f"âŒ è®€å–æª”æ¡ˆå¤±æ•— {json_file}: {e}")
        
        print(f"âœ… æ‰¾åˆ° {len(annotations)} å€‹æœ‰æ•ˆæ¨™è¨»æª”æ¡ˆ")
        return annotations
    
    def validate_annotation(self, data):
        """é©—è­‰æ¨™è¨»è³‡æ–™æ ¼å¼"""
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_fields = ['measurements', 'image_dimensions']
        
        for field in required_fields:
            if field not in data:
                return False
        
        # æª¢æŸ¥measurementså…§å®¹
        if not isinstance(data['measurements'], list) or len(data['measurements']) == 0:
            return False
        
        # æª¢æŸ¥æ¯å€‹measurementæ˜¯å¦åŒ…å«çµ‚æ¿è³‡è¨Š
        for m in data['measurements']:
            if 'lowerEndplate' not in m or 'upperEndplate' not in m:
                return False
            if len(m['lowerEndplate']) < 2 or len(m['upperEndplate']) < 2:
                return False
        
        return True
    
    def prepare_training_data(self, annotations):
        """æº–å‚™è¨“ç·´æ•¸æ“š"""
        print("ğŸ“ æº–å‚™è¨“ç·´æ•¸æ“š...")
        
        train_annotations = []
        val_annotations = []
        
        for i, ann in enumerate(tqdm(annotations, desc="è™•ç†æ¨™è¨»")):
            data = ann['data']
            json_file = ann['file']
            
            # è‡ªå‹•å°‹æ‰¾å°æ‡‰çš„DICOMæª”æ¡ˆ
            image_path = data.get('image_path', '')
            if not image_path or not os.path.exists(image_path):
                # å˜—è©¦æ‰¾åŒåçš„.dcmæª”æ¡ˆ
                json_path = Path(json_file)
                dcm_path = json_path.with_suffix('.dcm')
                
                if dcm_path.exists():
                    image_path = str(dcm_path.relative_to(self.input_dir))
                else:
                    # å˜—è©¦åœ¨åŒç›®éŒ„ä¸‹æ‰¾åŒådcm
                    base_name = json_path.stem
                    dcm_candidates = list(json_path.parent.glob(f'{base_name}*.dcm'))
                    if dcm_candidates:
                        image_path = str(dcm_candidates[0].relative_to(self.input_dir))
            
            # æå–è³‡è¨Š
            processed_data = {
                'patient_id': data.get('patient_id', ''),
                'study_id': data.get('study_id', ''),
                'study_date': data.get('study_date', ''),
                'spine_type': data.get('spine_type', 'L'),
                'image_type': data.get('image_type', 'neutral'),
                'image_path': image_path,
                'image_dimensions': data.get('image_dimensions', {}),
                'annotator': data.get('annotator', {}),
                'annotation_date': data.get('annotation_date', ''),
                'measurements': [],
                'vertebra_edges': data.get('vertebra_edges', {}),
                'clinical_notes': data.get('clinical_notes', {}),
                'surgery_info': data.get('surgery_info', {})
            }
            
            # è™•ç†measurements
            for m in data['measurements']:
                measurement = {
                    'level': m.get('level', ''),
                    'lowerEndplate': m.get('lowerEndplate', []),
                    'upperEndplate': m.get('upperEndplate', []),
                    'confidence': m.get('confidence', 0.95),
                    'measurement_method': m.get('measurement_method', 'manual')
                }
                
                # å¯é¸: ä¿ç•™è§’åº¦è³‡è¨Šä¾›åƒè€ƒ
                if 'angle' in m:
                    measurement['angle'] = m['angle']
                if 'angle_raw' in m:
                    measurement['angle_raw'] = m['angle_raw']
                
                processed_data['measurements'].append(measurement)
            
            # 80-20åˆ†å‰²
            if i % 5 == 0:
                val_annotations.append(processed_data)
            else:
                train_annotations.append(processed_data)
        
        # ä¿å­˜æ¨™è¨»æª”æ¡ˆ
        train_file = self.output_dir / 'annotations' / 'train_annotations.json'
        val_file = self.output_dir / 'annotations' / 'val_annotations.json'
        
        with open(train_file, 'w', encoding='utf-8') as f:
            json.dump(train_annotations, f, ensure_ascii=False, indent=2)
        
        with open(val_file, 'w', encoding='utf-8') as f:
            json.dump(val_annotations, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è¨“ç·´é›†: {len(train_annotations)} å€‹æ¨£æœ¬ -> {train_file}")
        print(f"âœ… é©—è­‰é›†: {len(val_annotations)} å€‹æ¨£æœ¬ -> {val_file}")
        
        return train_annotations, val_annotations
    
    def analyze_dataset(self, train_annotations, val_annotations):
        """åˆ†ææ•¸æ“šé›†çµ±è¨ˆè³‡è¨Š"""
        print("\nğŸ“Š æ•¸æ“šé›†åˆ†æ:")
        
        all_annotations = train_annotations + val_annotations
        
        # ç¸½æ¨£æœ¬æ•¸
        print(f"  ç¸½æ¨£æœ¬æ•¸: {len(all_annotations)}")
        print(f"  è¨“ç·´æ¨£æœ¬: {len(train_annotations)}")
        print(f"  é©—è­‰æ¨£æœ¬: {len(val_annotations)}")
        
        # è„Šæ¤é¡å‹çµ±è¨ˆ
        spine_types = {}
        for ann in all_annotations:
            spine_type = ann.get('spine_type', 'L')
            spine_types[spine_type] = spine_types.get(spine_type, 0) + 1
        print(f"\n  è„Šæ¤é¡å‹åˆ†å¸ƒ: {spine_types}")
        
        # å½±åƒé¡å‹çµ±è¨ˆ
        image_types = {}
        for ann in all_annotations:
            image_type = ann.get('image_type', 'neutral')
            image_types[image_type] = image_types.get(image_type, 0) + 1
        print(f"  å½±åƒé¡å‹åˆ†å¸ƒ: {image_types}")
        
        # æ¤é–“éš™çµ±è¨ˆ
        total_measurements = sum(len(ann['measurements']) for ann in all_annotations)
        avg_measurements = total_measurements / len(all_annotations)
        print(f"\n  ç¸½æ¤é–“éš™æ•¸: {total_measurements}")
        print(f"  å¹³å‡æ¯å¼µå½±åƒ: {avg_measurements:.1f} å€‹æ¤é–“éš™")
        
        # æ¤é–“éš™å±¤ç´šçµ±è¨ˆ
        level_counts = {}
        for ann in all_annotations:
            for m in ann['measurements']:
                level = m.get('level', '')
                level_counts[level] = level_counts.get(level, 0) + 1
        
        print(f"\n  æ¤é–“éš™å±¤ç´šåˆ†å¸ƒ:")
        for level in sorted(level_counts.keys()):
            print(f"    {level}: {level_counts[level]}")
        
        # çµ‚æ¿é»æ•¸çµ±è¨ˆ
        total_endplate_points = 0
        for ann in all_annotations:
            for m in ann['measurements']:
                total_endplate_points += len(m.get('lowerEndplate', [])) + len(m.get('upperEndplate', []))
        print(f"\n  ç¸½çµ‚æ¿é»æ•¸: {total_endplate_points}")
        
        # æ¤é«”é‚Šç·£çµ±è¨ˆ
        vertebra_with_edges = sum(1 for ann in all_annotations if ann.get('vertebra_edges'))
        total_edges = sum(len(ann.get('vertebra_edges', {})) for ann in all_annotations)
        print(f"\n  åŒ…å«æ¤é«”é‚Šç·£çš„æ¨£æœ¬: {vertebra_with_edges}")
        print(f"  ç¸½æ¤é«”é‚Šç·£æ•¸: {total_edges}")
        
        # æ‰‹è¡“è³‡è¨Šçµ±è¨ˆ
        surgery_cases = sum(1 for ann in all_annotations 
                          if ann.get('surgery_info', {}).get('surgery_done', False))
        print(f"\n  è¡“å¾Œç—…ä¾‹: {surgery_cases}/{len(all_annotations)}")
        
        # å½±åƒå°ºå¯¸çµ±è¨ˆ
        widths = [ann['image_dimensions'].get('width', 0) for ann in all_annotations]
        heights = [ann['image_dimensions'].get('height', 0) for ann in all_annotations]
        
        if widths and heights:
            print(f"\n  å½±åƒå°ºå¯¸ç¯„åœ:")
            print(f"    å¯¬åº¦: {min(widths)} - {max(widths)} (å¹³å‡: {np.mean(widths):.0f})")
            print(f"    é«˜åº¦: {min(heights)} - {max(heights)} (å¹³å‡: {np.mean(heights):.0f})")
    
    def create_dataset_info(self):
        """å‰µå»ºæ•¸æ“šé›†è³‡è¨Šæª”æ¡ˆ"""
        info = {
            "dataset_name": "Spine Endplate Detection Dataset",
            "description": "è„Šæ¤çµ‚æ¿æª¢æ¸¬æ©Ÿå™¨å­¸ç¿’æ•¸æ“šé›† - å°ˆæ³¨æ–¼çµ‚æ¿å‰å¾Œç·£æª¢æ¸¬",
            "version": "2.0",
            "created_by": "AI Assistant",
            "format": {
                "images": "DICOM/JPG",
                "annotations": "JSON",
                "coordinate_system": "pixel coordinates"
            },
            "tasks": [
                "endplate_segmentation",
                "vertebra_edge_detection",
                "keypoint_detection"
            ],
            "annotation_format": {
                "measurements": [
                    {
                        "level": "æ¤é–“éš™æ¨™ç±¤ (å¦‚ L4/L5)",
                        "lowerEndplate": "ä¸‹çµ‚æ¿2å€‹ç«¯é»åº§æ¨™ [{x, y}, {x, y}]",
                        "upperEndplate": "ä¸Šçµ‚æ¿2å€‹ç«¯é»åº§æ¨™ [{x, y}, {x, y}]",
                        "confidence": "ä¿¡å¿ƒåº¦ (0-1)"
                    }
                ],
                "vertebra_edges": {
                    "vertebra_name": {
                        "anterior": "å‰ç·£2å€‹ç«¯é»åº§æ¨™",
                        "posterior": "å¾Œç·£2å€‹ç«¯é»åº§æ¨™"
                    }
                }
            },
            "model_outputs": [
                "endplate_segmentation_mask",
                "vertebra_anterior_edge",
                "vertebra_posterior_edge",
                "endplate_keypoints"
            ],
            "notes": "æ©Ÿå™¨å­¸ç¿’æ¨¡å‹åªè² è²¬æª¢æ¸¬çµ‚æ¿å‰å¾Œç·£ï¼Œä¸è¨ˆç®—è§’åº¦"
        }
        
        with open(self.output_dir / 'dataset_info.json', 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        
        print("\nâœ… å‰µå»ºæ•¸æ“šé›†è³‡è¨Šæª”æ¡ˆ")
    
    def process_all(self):
        """è™•ç†æ‰€æœ‰æ•¸æ“š"""
        print("ğŸš€ é–‹å§‹çµ‚æ¿æª¢æ¸¬æ•¸æ“šæº–å‚™æµç¨‹...")
        
        # 1. æ”¶é›†æ¨™è¨»æª”æ¡ˆ
        annotations = self.collect_annotations()
        
        if not annotations:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨™è¨»æª”æ¡ˆ")
            return
        
        # 2. æº–å‚™è¨“ç·´æ•¸æ“š
        train_annotations, val_annotations = self.prepare_training_data(annotations)
        
        # 3. åˆ†ææ•¸æ“šé›†
        self.analyze_dataset(train_annotations, val_annotations)
        
        # 4. å‰µå»ºæ•¸æ“šé›†è³‡è¨Š
        self.create_dataset_info()
        
        print("\nğŸ‰ æ•¸æ“šæº–å‚™å®Œæˆ!")
        print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        print("ğŸ“‹ ç”Ÿæˆçš„æª”æ¡ˆ:")
        print("  - annotations/train_annotations.json")
        print("  - annotations/val_annotations.json")
        print("  - dataset_info.json")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='çµ‚æ¿æª¢æ¸¬æ•¸æ“šæº–å‚™')
    parser.add_argument('--input_dir', type=str, default='.',
                       help='æ¨™è¨»æª”æ¡ˆç›®éŒ„ï¼ˆé è¨­ç‚ºç•¶å‰ç›®éŒ„ï¼‰')
    parser.add_argument('--output_dir', type=str, default='endplate_training_data',
                       help='è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ç‚ºç•¶å‰ç›®éŒ„ä¸‹çš„ endplate_training_dataï¼‰')
    
    args = parser.parse_args()
    
    # å‰µå»ºæ•¸æ“šæº–å‚™å™¨
    preparer = EndplateDataPreparer(args.input_dir, args.output_dir)
    
    # è™•ç†æ‰€æœ‰æ•¸æ“š
    preparer.process_all()

if __name__ == "__main__":
    main()
